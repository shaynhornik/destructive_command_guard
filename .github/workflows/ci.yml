name: ci

on:
  pull_request:
  push:
    branches: [master, main]
  schedule:
    # Run deep suite daily at 3am UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    # Allow manual triggering for deep suite

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Validate Dependabot config
        run: |
          ruby -e 'require "yaml"; config = YAML.load_file(".github/dependabot.yml"); puts "Dependabot config:"; puts config.inspect'

      - uses: dtolnay/rust-toolchain@nightly
        with:
          components: rustfmt, clippy

      - name: Install cargo-nextest
        uses: taiki-e/install-action@nextest

      - name: Cache cargo registry and target
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock', '**/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Check formatting
        run: cargo fmt -- --check

      - name: Run clippy
        run: cargo clippy --all-targets -- -D warnings

      - name: Check compilation
        run: cargo check --all-targets

      - name: Run tests (with JUnit XML report)
        run: |
          cargo nextest run --profile ci --no-fail-fast

      - name: Generate test summary
        if: always()
        run: |
          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count pack tests
          PACK_TESTS=$(cargo test packs:: -- --list 2>/dev/null | grep -c "test$" || echo "0")
          TOTAL_TESTS=$(cargo test -- --list 2>/dev/null | grep -c "test$" || echo "0")

          echo "| Category | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total Tests | $TOTAL_TESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| Pack Tests | $PACK_TESTS |" >> $GITHUB_STEP_SUMMARY

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-check
          path: target/nextest/ci/junit.xml
          retention-days: 14
          if-no-files-found: ignore

      - name: Test Report Summary
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Unit Tests
          path: target/nextest/ci/junit.xml
          reporter: java-junit
          fail-on-error: false

  coverage:
    runs-on: ubuntu-latest
    needs: check
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@nightly
        with:
          components: llvm-tools-preview

      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov

      - name: Cache cargo registry and target
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-cov-${{ hashFiles('**/Cargo.lock', '**/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-cov-

      - name: Run tests with coverage
        run: |
          cargo llvm-cov --all-features --workspace \
            --ignore-filename-regex='(tests/|benches/|\.cargo/)' \
            --no-report

      - name: Generate coverage reports
        run: |
          # Generate LCOV from collected data (no re-running tests)
          cargo llvm-cov report --all-features --workspace \
            --ignore-filename-regex='(tests/|benches/|\.cargo/)' \
            --lcov --output-path lcov.info
          # Generate text summary from collected data
          cargo llvm-cov report --all-features --workspace \
            --ignore-filename-regex='(tests/|benches/|\.cargo/)' \
            --text > coverage-summary.txt
          echo "## Coverage Summary" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -20 coverage-summary.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage artifact
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            lcov.info
            coverage-summary.txt
          retention-days: 30

      - name: Check coverage thresholds (enforced)
        run: |
          set -euo pipefail

          OVERALL_MIN="70.0"
          EVALUATOR_MIN="80.0"
          HOOK_MIN="80.0"

          overall=$(grep 'TOTAL' coverage-summary.txt | grep -Eo '[0-9]+(\.[0-9]+)?%' | tail -1 | tr -d '%' || echo "")
          evaluator=$(awk '$0 ~ /src\/evaluator\.rs/ {for (i=NF; i>=1; i--) if ($i ~ /%$/) {print $i; break}}' coverage-summary.txt | tr -d '%' | tail -1 || true)
          hook=$(awk '$0 ~ /src\/hook\.rs/ {for (i=NF; i>=1; i--) if ($i ~ /%$/) {print $i; break}}' coverage-summary.txt | tr -d '%' | tail -1 || true)

          echo "Coverage thresholds:"
          echo "  overall >= ${OVERALL_MIN}%"
          echo "  src/evaluator.rs >= ${EVALUATOR_MIN}%"
          echo "  src/hook.rs >= ${HOOK_MIN}%"
          echo ""
          echo "Observed coverage:"
          echo "  overall=${overall}%"
          echo "  src/evaluator.rs=${evaluator}%"
          echo "  src/hook.rs=${hook}%"

          echo "coverage_overall=${overall}" >> "$GITHUB_OUTPUT"
          echo "coverage_evaluator=${evaluator}" >> "$GITHUB_OUTPUT"
          echo "coverage_hook=${hook}" >> "$GITHUB_OUTPUT"

          failures=0
          if [ -z "$overall" ]; then
            echo "::error::Failed to parse TOTAL coverage from coverage-summary.txt"
            failures=$((failures + 1))
          elif awk -v v="$overall" -v min="$OVERALL_MIN" 'BEGIN{exit !(v+0 < min+0)}'; then
            echo "::error::Overall coverage ${overall}% is below ${OVERALL_MIN}%"
            failures=$((failures + 1))
          fi

          if [ -z "$evaluator" ]; then
            echo "::error::Failed to parse src/evaluator.rs coverage from coverage-summary.txt"
            failures=$((failures + 1))
          elif awk -v v="$evaluator" -v min="$EVALUATOR_MIN" 'BEGIN{exit !(v+0 < min+0)}'; then
            echo "::error::src/evaluator.rs coverage ${evaluator}% is below ${EVALUATOR_MIN}%"
            failures=$((failures + 1))
          fi

          if [ -z "$hook" ]; then
            echo "::error::Failed to parse src/hook.rs coverage from coverage-summary.txt"
            failures=$((failures + 1))
          elif awk -v v="$hook" -v min="$HOOK_MIN" 'BEGIN{exit !(v+0 < min+0)}'; then
            echo "::error::src/hook.rs coverage ${hook}% is below ${HOOK_MIN}%"
            failures=$((failures + 1))
          fi

          if [ "$failures" -gt 0 ]; then
            echo "::error::Coverage thresholds not met (${failures} failure(s))"
            exit 1
          fi

          echo "Coverage thresholds satisfied."

  # Memory leak detection tests
  memory-tests:
    runs-on: ubuntu-latest
    needs: check
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@nightly

      - name: Cache cargo registry and target
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-memory-${{ hashFiles('**/Cargo.lock', '**/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-memory-

      - name: Run memory tests
        id: memory_tests
        run: |
          echo "=== DCG Memory Leak Tests ===" | tee memory-output.log
          echo "Timestamp: $(date -Iseconds)" | tee -a memory-output.log
          echo "Runner: ${{ runner.os }}" | tee -a memory-output.log
          echo "Rust: $(rustc --version)" | tee -a memory-output.log
          echo "" | tee -a memory-output.log

          # Get baseline system memory
          echo "=== System Memory Baseline ===" | tee -a memory-output.log
          free -h | tee -a memory-output.log
          echo "" | tee -a memory-output.log

          echo "=== Running Memory Tests ===" | tee -a memory-output.log
          # Memory tests must run sequentially for accurate measurements
          # Release mode for realistic performance characteristics
          if cargo test --test memory_tests --release -- --nocapture --test-threads=1 2>&1 | tee -a memory-output.log; then
            echo "" | tee -a memory-output.log
            echo "=== Memory Tests: ALL PASSED ===" | tee -a memory-output.log
            echo "memory_tests_result=passed" >> $GITHUB_OUTPUT
          else
            echo "" | tee -a memory-output.log
            echo "=== Memory Tests: FAILED ===" | tee -a memory-output.log
            echo "memory_tests_result=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Parse memory metrics
        if: always()
        run: |
          echo "=== Memory Test Metrics ===" | tee -a memory-metrics.log
          echo "" | tee -a memory-metrics.log

          # Extract metrics from test output
          echo "Test Results:" | tee -a memory-metrics.log
          grep -E "^memory_" memory-output.log | tee -a memory-metrics.log || echo "No metrics found" | tee -a memory-metrics.log

          echo "" | tee -a memory-metrics.log
          echo "Growth Summary:" | tee -a memory-metrics.log
          grep -E "final.*growth" memory-output.log | tee -a memory-metrics.log || echo "No growth data" | tee -a memory-metrics.log

          echo "" | tee -a memory-metrics.log
          echo "Pass/Fail:" | tee -a memory-metrics.log
          grep -E "(PASSED|FAILED|panicked)" memory-output.log | tee -a memory-metrics.log || echo "No status found" | tee -a memory-metrics.log

      - name: Memory test summary
        if: always()
        run: |
          echo "## Memory Leak Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Result badge
          if grep -q "ALL PASSED" memory-output.log; then
            echo "**Result:** ✅ All tests passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Result:** ❌ Tests failed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Metrics table
          echo "### Memory Growth by Test" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test | Final Growth | Limit | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------|--------------|-------|--------|" >> $GITHUB_STEP_SUMMARY

          # Parse and format metrics
          grep -E "final.*growth" memory-output.log | while read line; do
            test_name=$(echo "$line" | grep -oP "^[^:]+")
            growth=$(echo "$line" | grep -oP "growth: \\K[0-9]+ KB" || echo "?")
            limit=$(echo "$line" | grep -oP "limit: \\K[0-9]+ KB" || echo "?")
            if echo "$line" | grep -q "PASSED"; then
              status="✅"
            else
              status="❌"
            fi
            echo "| $test_name | $growth | $limit | $status |" >> $GITHUB_STEP_SUMMARY
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Full Output" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          tail -50 memory-output.log >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Upload memory test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: memory-test-output
          path: |
            memory-output.log
            memory-metrics.log
          retention-days: 14

  # Performance benchmark enforcement (push to master only)
  # Runs benchmarks and checks against performance budgets defined in src/perf.rs
  benchmarks:
    runs-on: ubuntu-latest
    needs: check
    # Only run on push to master/main, not on PRs (benchmarks are noisy)
    if: github.event_name == 'push' && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main')
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@nightly

      - name: Cache cargo registry and target
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock', '**/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-

      - name: Run benchmarks
        run: |
          # Run benchmarks and capture output
          cargo bench --bench heredoc_perf -- --noplot 2>&1 | tee benchmark_output.txt
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          grep -E "^(tier|pack|core|shell|language|full)" benchmark_output.txt | head -50 >> $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Check performance budgets
        run: |
          # Extract timing summaries and check against budgets
          # Budgets from src/perf.rs:
          # - Quick reject: 50μs panic
          # - Fast path: 500μs panic
          # - Pattern match: 1ms panic
          # - Heredoc extract: 2ms panic
          # - Full pipeline: 50ms panic
          echo "## Performance Budget Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          PANIC_VIOLATIONS=0

          # Check for any results exceeding 50ms (absolute max)
          if grep -E "time:.*\[.*[0-9]+\.[0-9]+ s" benchmark_output.txt; then
            echo "::error::Some benchmarks exceeded 1 second - major regression detected"
            PANIC_VIOLATIONS=$((PANIC_VIOLATIONS + 1))
          fi

          # Check full_pipeline benchmarks (budget: 50ms panic)
          if grep -A1 "full_pipeline" benchmark_output.txt | grep -E "time:.*\[.*[5-9][0-9]\.[0-9]+ ms|[0-9]{3,}\.[0-9]+ ms"; then
            echo "::warning::Full pipeline benchmark exceeds 50ms budget"
            PANIC_VIOLATIONS=$((PANIC_VIOLATIONS + 1))
          fi

          if [ $PANIC_VIOLATIONS -gt 0 ]; then
            echo "::error::$PANIC_VIOLATIONS performance budget violations detected"
            echo "Budget violations: $PANIC_VIOLATIONS" >> $GITHUB_STEP_SUMMARY
            # For now, warn but don't fail (benchmarks can be noisy in CI)
            # exit 1
          else
            echo "All benchmarks within budget" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_output.txt
          retention-days: 30

  # End-to-end shell script tests
  e2e:
    runs-on: ubuntu-latest
    needs: check
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@nightly

      - name: Cache cargo registry and target
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock', '**/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Build release binary
        run: cargo build --release

      - name: Run E2E tests
        run: |
          mkdir -p e2e-artifacts
          ./scripts/e2e_test.sh --verbose --binary target/release/dcg --json --artifacts e2e-artifacts 2>&1 | tee e2e_output.json
          echo "## E2E Test Results" >> $GITHUB_STEP_SUMMARY
          # Extract summary from JSON output
          if jq -e '.summary' e2e_output.json >/dev/null 2>&1; then
            echo '```json' >> $GITHUB_STEP_SUMMARY
            jq '.summary' e2e_output.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -20 e2e_output.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload E2E artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-failure-artifacts
          path: |
            e2e_output.json
            e2e-artifacts/
          retention-days: 14

  # Deep suite: fuzzing (scheduled or manual only)
  fuzz:
    runs-on: ubuntu-latest
    # Only run on schedule or manual trigger, not on every PR
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@nightly
        with:
          components: llvm-tools-preview

      - name: Install cargo-fuzz
        run: cargo install cargo-fuzz

      - name: Cache cargo registry and fuzz corpus
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            fuzz/corpus
            fuzz/artifacts
          key: ${{ runner.os }}-fuzz-${{ hashFiles('**/Cargo.lock', '**/Cargo.toml', 'fuzz/**') }}
          restore-keys: |
            ${{ runner.os }}-fuzz-

      - name: Run fuzz tests (time-limited)
        run: |
          cd fuzz
          echo "## Fuzzing Results" >> $GITHUB_STEP_SUMMARY
          for target in fuzz_context fuzz_evaluate fuzz_hook_input fuzz_normalize fuzz_heredoc_trigger fuzz_heredoc_extract fuzz_heredoc_language fuzz_shell_extract; do
            echo "Fuzzing $target (~60s runtime + build)..."
            timeout 10m cargo fuzz run "$target" -- -max_total_time=60 || true
            echo "- $target: completed" >> $GITHUB_STEP_SUMMARY
          done

      - name: Upload fuzz artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: fuzz-artifacts
          path: fuzz/artifacts/
          retention-days: 30
          if-no-files-found: ignore
